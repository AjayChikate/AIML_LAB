{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9qy0lBW75Cms",
        "Qcq0PGs93Kxk",
        "WrBgDaK64dmM",
        "ydwZgwHS3TDL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements"
      ],
      "metadata": {
        "id": "zYfik6FyNhZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- bert"
      ],
      "metadata": {
        "id": "eAXxqgmePSWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "284EpjyDTJxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-pretrained-bert pytorch-nlp"
      ],
      "metadata": {
        "id": "5bmGhiOkNk-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- speech"
      ],
      "metadata": {
        "id": "ZBqZmPG3PVkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg -y"
      ],
      "metadata": {
        "id": "F-M9lycbNk6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install portaudio19-dev # Linux-case\n",
        "!python -m pip install pyaudio\n",
        "!pip install SpeechRecognition\n",
        "!pip install pyttsx3\n",
        "!pip install --upgrade pyttsx3\n",
        "!pip uninstall pyttsx3\n",
        "!pip install pyttsx3==2.90\n",
        "!sudo apt-get install -y espeak\n",
        "!sudo apt-get update\n",
        "!pip install noisereduce soundfile\n",
        "!pip install jiwer"
      ],
      "metadata": {
        "id": "baeQ9RzTOeXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- multimodality -> #!pip install torch transformers sentence-transformers scikit-learn pandas opencv-python moviepy mediapipe"
      ],
      "metadata": {
        "id": "4i973RNXQ-db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAB 2"
      ],
      "metadata": {
        "id": "aSA0OQys5LOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP CLASSIFICATION BERT FINETUNING"
      ],
      "metadata": {
        "id": "9qy0lBW75Cms"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxFghUhtUgXZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences] #sentences is list of sentences we modify it by adding special tokens\n",
        "labels = df.label.values  #labels list of 0/1 classification for each such sentence\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # import model\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences] # list of tokens of sentences\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])\n",
        "\n",
        "MAX_LEN = 128\n",
        "#padding\n",
        "# maxlen=MAX_LEN: maximum allowed sentence length (e.g., 128, 256).\n",
        "# If a sentence is longer → truncate. If shorter → pad with 0s.\n",
        "# dtype=\"long\": output array type (64-bit integers).\n",
        "# truncating=\"post\": if sequence is too long, cut tokens from the end.\n",
        "# padding=\"post\": if sequence is too short, add zeros at the end.\n",
        "\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],#list of token ids\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "\n",
        "attention_masks = []   #Attention mask = tells BERT which tokens are real words and which are just padding.\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)   #a mask of 1s for each token followed by 0s for padding\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "\n",
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "#doing mini batch\n",
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,\n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "waprwFUgaBfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)  #num_labels=2 Defines how many classes we’re predicting.\n",
        "model.cuda() # to run on GPU If you don’t run this, training will default to CPU (much slower)."
      ],
      "metadata": {
        "id": "ln30Qebslcp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all model parameters with their names (e.g., 'bert.encoder.layer.0.attention.self.query.weight')\n",
        "param_optimizer = list(model.named_parameters())\n",
        "\n",
        "# These parameters should NOT be decayed (weight decay = L2 regularization)\n",
        "# 'bias', 'gamma', and 'beta' are common in BERT's LayerNorm and biases.\n",
        "# We don't want weight decay on them because it hurts performance.\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "\n",
        "# Create two groups of parameters for the optimizer:\n",
        "optimizer_grouped_parameters = [\n",
        "    # Group 1: Parameters that will have weight decay (regularization applied)\n",
        "    {\n",
        "        'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "        'weight_decay_rate': 0.01  # Apply L2 regularization\n",
        "    },\n",
        "    # Group 2: Parameters that will NOT have weight decay (biases, gamma, beta)\n",
        "    {\n",
        "        'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "        'weight_decay_rate': 0.0   # No regularization\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,\n",
        "                     warmup=.1)\n",
        "\n",
        "\n",
        "# Function to calculate the accuracy of predictions against true labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    # preds: model output probabilities/logits of shape (num_samples, num_classes)\n",
        "    # labels: true labels of shape (num_samples,)\n",
        "\n",
        "    # Step 1: Take the index of the maximum value in each prediction vector\n",
        "    # This gives us the predicted class for each sample\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()  # flatten() ensures it's 1D\n",
        "\n",
        "    # Step 2: Flatten the labels array to 1D as well\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    # Step 3: Compare predictions with labels, sum correct predictions\n",
        "    correct_predictions = np.sum(pred_flat == labels_flat)\n",
        "\n",
        "    # Step 4: Divide by total number of samples to get accuracy\n",
        "    accuracy = correct_predictions / len(labels_flat)\n",
        "\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "dduzcldRlcnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "\n",
        "\n",
        "  # Training\n",
        "\n",
        "  # Set our model to training mode (as opposed to evaluation mode)\n",
        "  model.train()\n",
        "\n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    train_loss_set.append(loss.item())\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "\n",
        "  # Validation\n",
        "\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables\n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
      ],
      "metadata": {
        "id": "verOSJqnlckj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "same for test and evaluation see from Lab2 ipynb"
      ],
      "metadata": {
        "id": "IbQBxE_ApgT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the metric\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Initialize a list to store MCC values for each batch\n",
        "matthews_set = []\n",
        "\n",
        "# Loop through each batch of predictions and true labels\n",
        "for i in range(len(true_labels)):\n",
        "    # Step 1: Get the predicted class for each sample in the batch\n",
        "    # np.argmax selects the class with the highest predicted probability\n",
        "    pred_classes = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "    # Step 2: Compute Matthew's correlation coefficient for this batch\n",
        "    # MCC accounts for true/false positives/negatives and works well for imbalanced datasets\n",
        "    matthews = matthews_corrcoef(true_labels[i], pred_classes)\n",
        "\n",
        "    # Step 3: Append the MCC for this batch to the list\n",
        "    matthews_set.append(matthews)\n"
      ],
      "metadata": {
        "id": "rWkm47oHlchs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AaWzbjC_lce0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAB 3  SPEECH TRANSLATION"
      ],
      "metadata": {
        "id": "wX5IEN2irmdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SPEECH TO TEXT"
      ],
      "metadata": {
        "id": "Qcq0PGs93Kxk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Builtin whisper BEST accuracy"
      ],
      "metadata": {
        "id": "DOLLcr9uwxsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Whisper + dependencies\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg -y\n",
        "\n",
        "# Import libraries\n",
        "import whisper\n",
        "\n",
        "# Load pre-trained Whisper model\n",
        "# Options: tiny, base, small, medium, large\n",
        "model = whisper.load_model(\"small\")\n",
        "\n",
        "# Upload an audio file (wav, mp3, m4a etc.)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Take first uploaded file\n",
        "audio_file = list(uploaded.keys())[0]\n",
        "\n",
        "# Transcribe the audio\n",
        "result = model.transcribe(audio_file)   #can pass optional parameter result = model.transcribe(audio_file, language=\"en\")\n",
        "\n",
        "# Print the transcription\n",
        "print(\"Transcription:\\n\", result[\"text\"])"
      ],
      "metadata": {
        "id": "blK0GVqBrrAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noise Reduction"
      ],
      "metadata": {
        "id": "nVHWdL_rxJLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- slides"
      ],
      "metadata": {
        "id": "uQMHvmP_zj34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " import noisereduce as nr\n",
        " from scipy.io import wavfile\n",
        "\n",
        " # 1. Load the noisy audio file and its sample rate\n",
        " rate, data = wavfile.read(\"my_noisy_audio.wav\")\n",
        "\n",
        " # 2. Select a sample of pure noise\n",
        " #(e.g., the first 1 second). This part is crucial!\n",
        " noise_clip = data[0:rate]\n",
        "\n",
        " # 3. Perform noise reduction\n",
        " reduced_noise_data = nr.reduce_noise(y=data, sr=rate, y_noise=noise_clip)\n",
        "\n",
        " # 4. Save the clean audio to a new file\n",
        " wavfile.write(\"my_clean_audio.wav\", rate, reduced_noise_data)\n",
        " print(\"Noise reduction complete!\")"
      ],
      "metadata": {
        "id": "izUWUdK4zHS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- assignment"
      ],
      "metadata": {
        "id": "sVSwWSjdzlkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import noisereduce as nr\n",
        "from scipy.io import wavfile\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load noisy audio\n",
        "rate, data = wavfile.read(\"my_noisy_audio.wav\")\n",
        "\n",
        "# Step 2: Convert stereo to mono if needed\n",
        "if len(data.shape) > 1:\n",
        "    data = np.mean(data, axis=1)  # average channels\n",
        "\n",
        "# Step 3: Normalize to float32 in range [-1, 1]\n",
        "# Avoids overflow during processing\n",
        "data = data.astype(np.float32)\n",
        "data = data / (np.max(np.abs(data)) + 1e-10)  # small epsilon to prevent division by 0\n",
        "\n",
        "# Step 4: Extract a pure noise segment (first 1 second)\n",
        "noise_clip = data[0:rate]\n",
        "\n",
        "# Step 5: Reduce noise\n",
        "reduced_noise_data = nr.reduce_noise(\n",
        "    y=data,\n",
        "    sr=rate,\n",
        "    y_noise=noise_clip,  # provide explicit noise profile\n",
        "    prop_decrease=1.0,   # adjust strength of noise reduction\n",
        ")\n",
        "\n",
        "# Step 6: Convert back to int16 for WAV file saving\n",
        "reduced_noise_data = np.int16(reduced_noise_data * 32767)\n",
        "\n",
        "# Step 7: Save cleaned audio\n",
        "wavfile.write(\"my_cleaned_audio.wav\", rate, reduced_noise_data)\n",
        "\n",
        "print(\"Noise reduction complete!\")\n"
      ],
      "metadata": {
        "id": "fo3Ox_NQzm6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NORMALISTION"
      ],
      "metadata": {
        "id": "CBLGlyDr1mj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# Load audio\n",
        "y, sr = librosa.load(\"my_noisy_audio.wav\", sr=16000)\n",
        "\n",
        "# Before normalization: max amplitude\n",
        "print(\"Before normalization:\", np.max(np.abs(y)))\n",
        "\n",
        "# Normalize to [-1, 1]\n",
        "y_norm = y / (np.max(np.abs(y)) + 1e-10)\n",
        "\n",
        "# After normalization\n",
        "print(\"After normalization:\", np.max(np.abs(y_norm)))\n"
      ],
      "metadata": {
        "id": "iorhDCeI1lCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Framing (25ms frames with 10ms hop)"
      ],
      "metadata": {
        "id": "zUASkf2I1v6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# Load normalized audio\n",
        "y, sr = librosa.load(\"my_noisy_audio.wav\", sr=16000)\n",
        "y = y / (np.max(np.abs(y)) + 1e-10)\n",
        "\n",
        "# Frame parameters\n",
        "frame_length = int(0.025 * sr)  # 25ms\n",
        "hop_length = int(0.010 * sr)    # 10ms overlap\n",
        "\n",
        "# Use librosa's framing\n",
        "frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length).T\n",
        "print(\"Shape of frames:\", frames.shape)  # (#frames, frame_length)\n"
      ],
      "metadata": {
        "id": "JIR9Fya31lVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization (Waveform + Spectrogram)"
      ],
      "metadata": {
        "id": "5dtquzCK2FJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load audio\n",
        "y, sr = librosa.load(\"my_noisy_audio.wav\", sr=16000)\n",
        "\n",
        "# Plot waveform\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveshow(y, sr=sr)\n",
        "plt.title(\"Audio Waveform\")\n",
        "plt.show()\n",
        "\n",
        "# Plot spectrogram\n",
        "D = librosa.stft(y)  # Short-time Fourier transform\n",
        "S_db = librosa.amplitude_to_db(abs(D), ref=np.max)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='hz')\n",
        "plt.colorbar(format=\"%+2.0f dB\")\n",
        "plt.title(\"Spectrogram\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oOp14j861loH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- MANUAL"
      ],
      "metadata": {
        "id": "qqYnWrBm2vrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import speech_recognition as sr  # For speech-to-text conversion\n",
        "import pyttsx3                  # For text-to-speech conversion\n",
        "\n",
        "# Step 1: Initialize the recognizer\n",
        "r = sr.Recognizer()  # This creates a Recognizer object which will process the audio\n",
        "\n",
        "# Step 2: Define a function to convert text to speech\n",
        "def SpeakText(command):\n",
        "    \"\"\"\n",
        "    Converts the input text (command) to spoken audio.\n",
        "    \"\"\"\n",
        "    engine = pyttsx3.init()  # Initialize the TTS engine\n",
        "    engine.say(command)       # Queue the text to speak\n",
        "    engine.runAndWait()       # Speak the text and wait until finished\n",
        "\n",
        "# Step 3: Specify the path to your audio file\n",
        "audio_file = \"/content/my_cleaned_audio.wav\"  # Replace with your cleaned audio file\n",
        "\n",
        "# Step 4: Open the audio file and read its content\n",
        "with sr.AudioFile(audio_file) as source:\n",
        "    audio_data = r.record(source)  # Load the entire audio file into memory\n",
        "\n",
        "    # Step 5: Convert speech to text using Google Speech Recognition\n",
        "    try:\n",
        "        MyText = r.recognize_google(audio_data)  # Transcribe the audio\n",
        "        MyText_Cleaned = MyText.lower()          # Convert text to lowercase for consistency\n",
        "        print(\"Did you say in after cleaning noisy audio:\", MyText_Cleaned)\n",
        "\n",
        "        # Step 6: Speak the recognized text aloud\n",
        "        SpeakText(MyText_Cleaned)\n",
        "\n",
        "    # Step 7: Handle possible errors\n",
        "    except sr.RequestError as e:\n",
        "        # API was unreachable or unresponsive\n",
        "        print(\"Could not request results; {0}\".format(e))\n",
        "    except sr.UnknownValueError:\n",
        "        # Speech was unintelligible\n",
        "        print(\"Unknown error occurred\")\n"
      ],
      "metadata": {
        "id": "54zx6XpF2vUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- ACCURACY"
      ],
      "metadata": {
        "id": "d-NWJmWq35EO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- LAB3 assignment wer builtin"
      ],
      "metadata": {
        "id": "BDMBUJhX4YIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WER IMPLEMENTATION"
      ],
      "metadata": {
        "id": "WrBgDaK64dmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wer(reference, hypothesis):\n",
        "    \"\"\"\n",
        "    Calculate Word Error Rate (WER) between reference and hypothesis text.\n",
        "\n",
        "    WER = (S + D + I) / N\n",
        "    S = substitutions, D = deletions, I = insertions, N = number of words in reference\n",
        "\n",
        "    Args:\n",
        "        reference (str): Correct text (ground truth)\n",
        "        hypothesis (str): Transcribed text\n",
        "\n",
        "    Returns:\n",
        "        float: WER as a fraction (0.0 to 1.0)\n",
        "    \"\"\"\n",
        "    # Step 1: Split sentences into lists of words\n",
        "    ref_words = reference.strip().split()\n",
        "    hyp_words = hypothesis.strip().split()\n",
        "\n",
        "    # Step 2: Initialize a matrix (len(ref)+1 x len(hyp)+1) for dynamic programming\n",
        "    # dp[i][j] = minimum edit distance between first i words of ref and first j words of hyp\n",
        "    n = len(ref_words)\n",
        "    m = len(hyp_words)\n",
        "    dp = [[0] * (m + 1) for _ in range(n + 1)]\n",
        "\n",
        "    # Step 3: Initialize base cases\n",
        "    for i in range(n + 1):\n",
        "        dp[i][0] = i  # i deletions\n",
        "    for j in range(m + 1):\n",
        "        dp[0][j] = j  # j insertions\n",
        "\n",
        "    # Step 4: Fill the matrix using Levenshtein distance\n",
        "    for i in range(1, n + 1):\n",
        "        for j in range(1, m + 1):\n",
        "            if ref_words[i - 1] == hyp_words[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1]  # no error if words match\n",
        "            else:\n",
        "                substitution = dp[i - 1][j - 1] + 1\n",
        "                insertion = dp[i][j - 1] + 1\n",
        "                deletion = dp[i - 1][j] + 1\n",
        "                dp[i][j] = min(substitution, insertion, deletion)\n",
        "\n",
        "    # Step 5: Total errors = dp[n][m]\n",
        "    errors = dp[n][m]\n",
        "\n",
        "    # Step 6: WER = errors / number of words in reference\n",
        "    wer_value = errors / n if n > 0 else 0.0\n",
        "\n",
        "    return wer_value\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Example usage\n",
        "reference_text = \"hello world this is a test\"\n",
        "hypothesis_text_noisy = \"hello word this is test\"\n",
        "hypothesis_text_cleaned = \"hello world this is a test\"\n",
        "\n",
        "print(f\"WER for noisy text:   {wer(reference_text, hypothesis_text_noisy) * 100:.2f}%\")\n",
        "print(f\"WER for cleaned text: {wer(reference_text, hypothesis_text_cleaned) * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "n0tIwB0a34vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEXT TO SPEECH"
      ],
      "metadata": {
        "id": "ydwZgwHS3TDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the text-to-speech library\n",
        "import pyttsx3\n",
        "\n",
        "# Step 1: Initialize the TTS engine\n",
        "engine = pyttsx3.init()  # creates a TTS engine object\n",
        "\n",
        "# Step 2: Set properties (optional)\n",
        "engine.setProperty('rate', 150)     # Speech rate (words per minute)\n",
        "engine.setProperty('volume', 1.0)   # Volume (0.0 to 1.0)\n",
        "voices = engine.getProperty('voices')\n",
        "engine.setProperty('voice', voices[0].id)  # Choose a voice (0 = first voice, 1 = second, etc.)\n",
        "\n",
        "# Step 3: Input text to speak\n",
        "text_to_speak = \"Hello! This is your text-to-speech program. You can type any text here.\"\n",
        "\n",
        "# Step 4: Speak the text\n",
        "engine.say(text_to_speak)\n",
        "\n",
        "# Step 5: Run the speech engine\n",
        "engine.runAndWait()\n",
        "\n",
        "print(\"Text has been spoken successfully!\")\n"
      ],
      "metadata": {
        "id": "dRVtNCbA1lzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ycEKToXi33Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iFN_W1WS33Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2f7_aYcy5RIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAB 4"
      ],
      "metadata": {
        "id": "C5Gmoh1S5S1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- Opencv image preprocessing"
      ],
      "metadata": {
        "id": "4hTPn2w2-7BC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- Object detection"
      ],
      "metadata": {
        "id": "H89j4rjnCHG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- image segmentation"
      ],
      "metadata": {
        "id": "sj5WR7SUCiHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- yolo live video py file"
      ],
      "metadata": {
        "id": "mSg5DgVgFFDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAB 5"
      ],
      "metadata": {
        "id": "l0bGwndbG0Su"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- multimodality assignment"
      ],
      "metadata": {
        "id": "NiIKBWJIREDK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e8GiCB6ZSkzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- gestures"
      ],
      "metadata": {
        "id": "WCybpEYCSluZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "gestures.py\n",
        "\n",
        "Heuristic gesture detectors for MediaPipe-like 21 hand landmarks.\n",
        "\n",
        "Each function expects `landmarks` to be a sequence of 21 points:\n",
        "    landmarks[i] == (x, y)  OR (x, y, z)\n",
        "Coordinates should be in the same units (e.g., normalized image coords or pixels).\n",
        "Indices follow MediaPipe Hands convention:\n",
        "    0: wrist\n",
        "    Thumb: 1..4 (tip = 4)\n",
        "    Index: 5..8 (tip = 8)\n",
        "    Middle: 9..12 (tip = 12)\n",
        "    Ring: 13..16 (tip = 16)\n",
        "    Pinky: 17..20 (tip = 20)\n",
        "\n",
        "Heuristics adapted from MediaPipe docs and community examples.\n",
        "References: MediaPipe Hands & community code.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Sequence, Tuple\n",
        "import math\n",
        "\n",
        "Point = Tuple[float, float]  # (x, y) or (x, y, z) - code uses only x,y\n",
        "\n",
        "# landmark index constants (MediaPipe)\n",
        "WRIST = 0\n",
        "THUMB_TIP = 4\n",
        "THUMB_IP = 3\n",
        "THUMB_MCP = 2\n",
        "INDEX_PIP = 6\n",
        "INDEX_TIP = 8\n",
        "MIDDLE_PIP = 10\n",
        "MIDDLE_TIP = 12\n",
        "RING_PIP = 14\n",
        "RING_TIP = 16\n",
        "PINKY_PIP = 18\n",
        "PINKY_TIP = 20\n",
        "\n",
        "TIP_INDICES = [THUMB_TIP, INDEX_TIP, MIDDLE_TIP, RING_TIP, PINKY_TIP]\n",
        "PIP_INDICES = [THUMB_IP, INDEX_PIP, MIDDLE_PIP, RING_PIP, PINKY_PIP]\n",
        "\n",
        "\n",
        "def _xy(pt):\n",
        "    \"\"\"Return (x,y) from a point that may have 2 or 3 elements.\"\"\"\n",
        "    return (pt[0], pt[1])\n",
        "\n",
        "\n",
        "def _dist(a: Point, b: Point) -> float:\n",
        "    ax, ay = _xy(a); bx, by = _xy(b)\n",
        "    return math.hypot(ax - bx, ay - by)\n",
        "\n",
        "\n",
        "def hand_size(landmarks: Sequence[Tuple[float, float]]) -> float:\n",
        "    \"\"\"\n",
        "    Estimate a scale for the hand using max distance between wrist and tips.\n",
        "    Used to make thresholds relative to hand size.\n",
        "    \"\"\"\n",
        "    wrist = landmarks[WRIST]\n",
        "    dists = [_dist(wrist, landmarks[i]) for i in TIP_INDICES]\n",
        "    maxd = max(dists) if dists else 1.0\n",
        "    return maxd\n",
        "\n",
        "\n",
        "def is_finger_extended(landmarks: Sequence[Tuple[float, float]],\n",
        "                       tip_idx: int, pip_idx: int,\n",
        "                       wrist_idx: int = WRIST,\n",
        "                       margin: float = 0.0) -> bool:\n",
        "    \"\"\"\n",
        "    Heuristic: a finger is considered extended if its tip is *farther* from the wrist\n",
        "    than its pip joint is. margin is an absolute value added to pip->wrist distance\n",
        "    to avoid borderline cases. Works reasonably well for upright and rotated hands.\n",
        "    \"\"\"\n",
        "    wrist = landmarks[wrist_idx]\n",
        "    tip = landmarks[tip_idx]\n",
        "    pip = landmarks[pip_idx]\n",
        "    d_tip = _dist(tip, wrist)\n",
        "    d_pip = _dist(pip, wrist)\n",
        "    return d_tip > (d_pip + margin)\n",
        "\n",
        "\n",
        "def fingers_extended_list(landmarks: Sequence[Tuple[float, float]],\n",
        "                          margin_ratio: float = 0.08) -> list:\n",
        "    \"\"\"\n",
        "    Return list of booleans [thumb, index, middle, ring, pinky] whether each finger\n",
        "    appears extended. margin_ratio multiplies hand_size to compute an absolute margin.\n",
        "    \"\"\"\n",
        "    size = hand_size(landmarks)\n",
        "    margin = size * margin_ratio\n",
        "    results = []\n",
        "    # Thumb uses TIP vs IP\n",
        "    results.append(is_finger_extended(landmarks, THUMB_TIP, THUMB_IP, margin=margin))\n",
        "    # Other fingers use tip vs pip\n",
        "    results.append(is_finger_extended(landmarks, INDEX_TIP, INDEX_PIP, margin=margin))\n",
        "    results.append(is_finger_extended(landmarks, MIDDLE_TIP, MIDDLE_PIP, margin=margin))\n",
        "    results.append(is_finger_extended(landmarks, RING_TIP, RING_PIP, margin=margin))\n",
        "    results.append(is_finger_extended(landmarks, PINKY_TIP, PINKY_PIP, margin=margin))\n",
        "    return results\n",
        "\n",
        "\n",
        "def is_palm_open(landmarks: Sequence[Tuple[float, float]],\n",
        "                 min_extended: int = 4,\n",
        "                 spread_ratio: float = 0.25) -> bool:\n",
        "    \"\"\"\n",
        "    Open palm: most fingers extended and fingertips are reasonably spread out.\n",
        "    - min_extended: minimum number of fingers extended (default 4, allows thumb fold)\n",
        "    - spread_ratio: min average tip-to-tip distance normalized by hand_size\n",
        "    \"\"\"\n",
        "    extended = fingers_extended_list(landmarks)\n",
        "    if sum(1 for e in extended if e) < min_extended:\n",
        "        return False\n",
        "\n",
        "    # compute average pairwise tip distance to check spread\n",
        "    tips = [landmarks[i] for i in TIP_INDICES]\n",
        "    n = len(tips)\n",
        "    pairwise = 0.0\n",
        "    count = 0\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            pairwise += _dist(tips[i], tips[j])\n",
        "            count += 1\n",
        "    avg_pair = (pairwise / count) if count else 0.0\n",
        "    size = hand_size(landmarks)\n",
        "    return avg_pair > (size * spread_ratio)\n",
        "\n",
        "\n",
        "def is_fist(landmarks: Sequence[Tuple[float, float]],\n",
        "            max_extended: int = 0,\n",
        "            near_wrist_ratio: float = 0.55) -> bool:\n",
        "    \"\"\"\n",
        "    Fist: no fingers extended and fingertips are near the wrist.\n",
        "    - max_extended: maximum allowed extended fingers (default 0)\n",
        "    - near_wrist_ratio: maximum allowed average tip->wrist distance relative to hand_size\n",
        "    \"\"\"\n",
        "    extended = fingers_extended_list(landmarks)\n",
        "    if sum(1 for e in extended if e) > max_extended:\n",
        "        return False\n",
        "\n",
        "    wrist = landmarks[WRIST]\n",
        "    avg_tip_dist = sum(_dist(wrist, landmarks[i]) for i in TIP_INDICES) / len(TIP_INDICES)\n",
        "    size = hand_size(landmarks)\n",
        "    return avg_tip_dist < (size * near_wrist_ratio)\n",
        "\n",
        "\n",
        "def is_thumb_up(landmarks: Sequence[Tuple[float, float]],\n",
        "                other_finger_max_extended: int = 0) -> bool:\n",
        "    \"\"\"\n",
        "    Thumb-up:\n",
        "     - thumb appears extended (tip farther from wrist than IP/MCP)\n",
        "     - other fingers are folded\n",
        "    This heuristic does not try to determine global 'up' direction (camera-dependent),\n",
        "    it only checks the thumb vs other fingers.\n",
        "    \"\"\"\n",
        "    extended = fingers_extended_list(landmarks)\n",
        "    thumb_ext = extended[0]\n",
        "    others_ext = sum(1 for e in extended[1:] if e)\n",
        "    return thumb_ext and (others_ext <= other_finger_max_extended)\n",
        "\n",
        "\n",
        "def is_ok_sign(landmarks: Sequence[Tuple[float, float]],\n",
        "               close_ratio: float = 0.12,\n",
        "               other_extended_min: int = 2) -> bool:\n",
        "    \"\"\"\n",
        "    OK sign: thumb tip and index tip are close (pinch circle) while other fingers are extended.\n",
        "    - close_ratio: max allowed distance between thumb_tip and index_tip relative to hand_size\n",
        "    - other_extended_min: minimum number of other fingers extended (e.g., middle+ring)\n",
        "    \"\"\"\n",
        "    size = hand_size(landmarks)\n",
        "    d = _dist(landmarks[THUMB_TIP], landmarks[INDEX_TIP])\n",
        "    extended = fingers_extended_list(landmarks)\n",
        "    others = extended[2:]  # middle, ring, pinky\n",
        "    return (d < size * close_ratio) and (sum(1 for e in others if e) >= other_extended_min)\n",
        "\n",
        "\n",
        "def is_peace_sign(landmarks: Sequence[Tuple[float, float]],\n",
        "                  required_extended = (False, True, True, False, False)) -> bool:\n",
        "    \"\"\"\n",
        "    Peace (V) sign: index & middle extended; ring & pinky folded. Thumb often folded or aside.\n",
        "    required_extended is a tuple of booleans for (thumb, index, middle, ring, pinky)\n",
        "    \"\"\"\n",
        "    extended = fingers_extended_list(landmarks)\n",
        "    for i, req in enumerate(required_extended):\n",
        "        if req and not extended[i]:\n",
        "            return False\n",
        "        if (not req) and extended[i] and i in (3, 4):  # ring/pinky should be folded\n",
        "            return False\n",
        "    # Additional check: index and middle tips should be separated enough\n",
        "    d_idx_mid = _dist(landmarks[INDEX_TIP], landmarks[MIDDLE_TIP])\n",
        "    size = hand_size(landmarks)\n",
        "    return d_idx_mid > (size * 0.12) and extended[1] and extended[2]\n",
        "\n",
        "\n",
        "def is_pointing(landmarks: Sequence[Tuple[float, float]],\n",
        "                require_index_extended: bool = True) -> bool:\n",
        "    \"\"\"\n",
        "    Pointing: index extended, other fingers folded (thumb may be either).\n",
        "    \"\"\"\n",
        "    extended = fingers_extended_list(landmarks)\n",
        "    index_ok = extended[1] if require_index_extended else True\n",
        "    others_folded = sum(1 for e in (extended[2], extended[3], extended[4]) if e) == 0\n",
        "    return index_ok and others_folded\n",
        "\n",
        "\n",
        "def is_pinch(landmarks: Sequence[Tuple[float, float]],\n",
        "             close_ratio: float = 0.1) -> bool:\n",
        "    \"\"\"\n",
        "    Pinch: thumb tip close to index tip (distance < threshold).\n",
        "    \"\"\"\n",
        "    size = hand_size(landmarks)\n",
        "    d = _dist(landmarks[THUMB_TIP], landmarks[INDEX_TIP])\n",
        "    return d < (size * close_ratio)\n",
        "\n",
        "\n",
        "# Convenience: detect a set of common gestures\n",
        "def detect_all(landmarks: Sequence[Tuple[float, float]]) -> dict:\n",
        "    \"\"\"\n",
        "    Returns a dict of detections { 'palm_open': bool, 'fist': bool, ... }\n",
        "    Use these signals together as you like.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'palm_open': is_palm_open(landmarks),\n",
        "        'fist': is_fist(landmarks),\n",
        "        'thumb_up': is_thumb_up(landmarks),\n",
        "        'ok': is_ok_sign(landmarks),\n",
        "        'peace': is_peace_sign(landmarks),\n",
        "        'pointing': is_pointing(landmarks),\n",
        "        'pinch': is_pinch(landmarks),\n",
        "        'extended_flags': fingers_extended_list(landmarks)\n",
        "    }\n",
        "\n",
        "# If used as script, show a tiny demo of expected input format\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"gestures.py loaded - functions ready.\")\n",
        "    print(\"Example: call detect_all(landmarks) where landmarks is a list of 21 (x,y) points.\")\n"
      ],
      "metadata": {
        "id": "MeYGsTb9-4FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to use\n",
        "\n",
        "Run MediaPipe Hands or other hand landmark detector and obtain the 21 landmarks per hand (MediaPipe returns normalized x,y). See MediaPipe docs for how to get landmarks.\n",
        "Google AI for Developers\n",
        "+1\n",
        "\n",
        "Pass the landmarks list to detect_all(landmarks) or any of the predicate functions above. Example:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LDZDq4RMSUHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pseudo-code\n",
        "landmarks = [(x0,y0), (x1,y1), ...]   # 21 items from MediaPipe\n",
        "results = detect_all(landmarks)\n",
        "if results['palm_open']:\n",
        "    print(\"Open palm detected\")"
      ],
      "metadata": {
        "id": "ErrXDColSW6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "pose_gestures.py\n",
        "\n",
        "Rule-based pose detectors using MediaPipe Pose landmarks.\n",
        "Requires mediapipe and opencv.\n",
        "\n",
        "Landmark indices (MediaPipe Pose, 33 points):\n",
        "    0: nose\n",
        "    11: left_shoulder\n",
        "    12: right_shoulder\n",
        "    13: left_elbow\n",
        "    14: right_elbow\n",
        "    15: left_wrist\n",
        "    16: right_wrist\n",
        "    23: left_hip\n",
        "    24: right_hip\n",
        "    25: left_knee\n",
        "    26: right_knee\n",
        "    27: left_ankle\n",
        "    28: right_ankle\n",
        "\"\"\"\n",
        "\n",
        "from typing import Sequence, Tuple\n",
        "import math\n",
        "\n",
        "Point = Tuple[float, float]\n",
        "\n",
        "# landmark indices\n",
        "NOSE = 0\n",
        "LEFT_SHOULDER, RIGHT_SHOULDER = 11, 12\n",
        "LEFT_ELBOW, RIGHT_ELBOW = 13, 14\n",
        "LEFT_WRIST, RIGHT_WRIST = 15, 16\n",
        "LEFT_HIP, RIGHT_HIP = 23, 24\n",
        "LEFT_KNEE, RIGHT_KNEE = 25, 26\n",
        "LEFT_ANKLE, RIGHT_ANKLE = 27, 28\n",
        "\n",
        "\n",
        "def _dist(a: Point, b: Point) -> float:\n",
        "    ax, ay = a; bx, by = b\n",
        "    return math.hypot(ax - bx, ay - by)\n",
        "\n",
        "\n",
        "def body_size(landmarks: Sequence[Point]) -> float:\n",
        "    \"\"\"\n",
        "    Use shoulder width as a reference scale.\n",
        "    \"\"\"\n",
        "    ls, rs = landmarks[LEFT_SHOULDER], landmarks[RIGHT_SHOULDER]\n",
        "    return _dist(ls, rs)\n",
        "\n",
        "\n",
        "def is_hands_up(landmarks: Sequence[Point]) -> bool:\n",
        "    \"\"\"\n",
        "    True if both wrists are above shoulders.\n",
        "    \"\"\"\n",
        "    ls, rs = landmarks[LEFT_SHOULDER], landmarks[RIGHT_SHOULDER]\n",
        "    lw, rw = landmarks[LEFT_WRIST], landmarks[RIGHT_WRIST]\n",
        "\n",
        "    return (lw[1] < ls[1]) and (rw[1] < rs[1])\n",
        "\n",
        "\n",
        "def is_t_pose(landmarks: Sequence[Point], tolerance: float = 0.15) -> bool:\n",
        "    \"\"\"\n",
        "    Arms extended sideways (T-pose).\n",
        "    Heuristic: wrists are roughly at shoulder height, far from torso.\n",
        "    \"\"\"\n",
        "    size = body_size(landmarks)\n",
        "    ls, rs = landmarks[LEFT_SHOULDER], landmarks[RIGHT_SHOULDER]\n",
        "    lw, rw = landmarks[LEFT_WRIST], landmarks[RIGHT_WRIST]\n",
        "\n",
        "    # y-level check\n",
        "    y_ok = (abs(lw[1] - ls[1]) < tolerance) and (abs(rw[1] - rs[1]) < tolerance)\n",
        "\n",
        "    # distance check (wrists away from shoulders)\n",
        "    x_ok = (abs(lw[0] - ls[0]) > size * 0.8) and (abs(rw[0] - rs[0]) > size * 0.8)\n",
        "\n",
        "    return y_ok and x_ok\n",
        "\n",
        "\n",
        "def is_squat(landmarks: Sequence[Point], knee_ratio: float = 0.8) -> bool:\n",
        "    \"\"\"\n",
        "    Squat: hips lowered close to knee height.\n",
        "    \"\"\"\n",
        "    lh, rh = landmarks[LEFT_HIP], landmarks[RIGHT_HIP]\n",
        "    lk, rk = landmarks[LEFT_KNEE], landmarks[RIGHT_KNEE]\n",
        "\n",
        "    avg_hip_y = (lh[1] + rh[1]) / 2\n",
        "    avg_knee_y = (lk[1] + rk[1]) / 2\n",
        "\n",
        "    return avg_hip_y > (avg_knee_y * knee_ratio)\n",
        "\n",
        "\n",
        "def is_standing(landmarks: Sequence[Point], straight_ratio: float = 0.3) -> bool:\n",
        "    \"\"\"\n",
        "    Standing straight: hips above knees, knees above ankles.\n",
        "    \"\"\"\n",
        "    lh, rh = landmarks[LEFT_HIP], landmarks[RIGHT_HIP]\n",
        "    lk, rk = landmarks[LEFT_KNEE], landmarks[RIGHT_KNEE]\n",
        "    la, ra = landmarks[LEFT_ANKLE], landmarks[RIGHT_ANKLE]\n",
        "\n",
        "    return (lh[1] < lk[1] < la[1]) and (rh[1] < rk[1] < ra[1])\n",
        "\n",
        "\n",
        "def detect_all(landmarks: Sequence[Point]) -> dict:\n",
        "    \"\"\"\n",
        "    Returns dict of detected poses.\n",
        "    Input landmarks should be normalized (0–1) from MediaPipe.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"hands_up\": is_hands_up(landmarks),\n",
        "        \"t_pose\": is_t_pose(landmarks),\n",
        "        \"squat\": is_squat(landmarks),\n",
        "        \"standing\": is_standing(landmarks)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "LX8WQlUoS6_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}